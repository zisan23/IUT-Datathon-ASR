{"cells":[{"cell_type":"markdown","source":["# Step 3. Training\n","This notebook trains the punctuation model. The training dataset after steps 1 and 2 can also be found at the following URL. https://www.kaggle.com/datasets/takuji/punctuation-model-dataset"],"metadata":{"id":"aHuTdIOTjb8q"}},{"cell_type":"markdown","metadata":{"id":"1BO-G86RjbNO"},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"2tJJWAdijbNW"},"outputs":[],"source":["import os\n","import json\n","import datetime\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","\n","import torch\n","import torch.nn as nn\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, get_linear_schedule_with_warmup\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"CINW78COjbNX"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxbYEiw9jbNZ"},"outputs":[],"source":["class CFG:\n","    batch_size=32\n","    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    tokenizer=AutoTokenizer.from_pretrained('xlm-roberta-large')\n","    full_finetuning=True\n","    epochs=2\n","    max_grad_norm=1.0\n","    checkpoint_dir='checkpoints'\n","    log_dir='runs'\n","    load_checkpoint=False\n","    checkpoint_path='checkpoint_last.pt'\n","    learning_rate=8e-6\n","    apex=True\n","\n","def process_data(is_train):\n","    if is_train:\n","        df = pd.read_csv('given_train.csv')\n","        df = pd.concat([df, pd.read_csv('indiccorpv2_0_train.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('indiccorpv2_0_valid.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('given_train.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('indiccorpv2_1_train.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('indiccorpv2_1_valid.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('given_train.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('indiccorpv2_2_train.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('indiccorpv2_2_valid.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('given_train.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('indiccorpv2_3_train.csv')]).reset_index(drop=True)\n","        df = pd.concat([df, pd.read_csv('indiccorpv2_3_valid.csv')]).reset_index(drop=True)\n","    else:\n","        df = pd.read_csv('given_valid.csv')\n","    df.dropna(inplace = True)\n","    tag_values = ['blank', 'end', 'comma', 'qm', 'hyp']\n","    tag_values.append(\"PAD\")\n","    encoder = {t: i for i, t in enumerate(tag_values)}\n","    print(f\"Encoder: {encoder}\")\n","    sentences = df['sentence'].values\n","    labels = df['label'].values\n","    return sentences, labels, encoder, tag_values\n","\n","def folder_with_time_stamps(log_folder, checkpoint_folder):\n","    folder_hook = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n","    log_saving = log_folder + '/' + folder_hook\n","    checkpoint_saving = checkpoint_folder + '/' + folder_hook\n","    train_encoder_file_path = 'label_encoder_' + folder_hook + '.json'\n","    return log_saving, checkpoint_saving, train_encoder_file_path, folder_hook\n","\n","log_folder, checkpoint_folder, train_encoder_file_path, _ = folder_with_time_stamps(CFG.log_dir,\n","                                                                                    CFG.checkpoint_dir)\n","\n","print(f\"train encoder path -> {train_encoder_file_path}\")\n","\n","os.makedirs(log_folder, exist_ok=True)\n","os.makedirs(checkpoint_folder, exist_ok=True)\n","\n","train_sentences, train_labels, train_encoder, tag_values = process_data(is_train=True)\n","valid_sentences, valid_labels, _, _ = process_data(is_train=False)\n","\n","with open(train_encoder_file_path, \"w\") as outfile:\n","    json.dump(train_encoder, outfile)\n","\n","print(\"--------------------------------Tag Values----------------------------------\")\n","print(tag_values)\n","\n","class PunctuationDataset(torch.utils.data.Dataset):\n","    def __init__(self, texts, labels, tag2idx):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tag2idx = tag2idx\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, item):\n","        sentence = self.texts[item].split()\n","        text_label = self.labels[item].split()\n","\n","        tokenized_sentence = []\n","        labels = []\n","\n","        for word, label in zip(sentence, text_label):\n","            # Tokenize the word and count number of subwords\n","            tokenized_word = CFG.tokenizer.tokenize(word)\n","            n_subwords = len(tokenized_word)\n","\n","            # Add the tokenized word to the final tokenized word list\n","            tokenized_sentence.extend(tokenized_word)\n","\n","            # Add the same label to the new list of labels `n_subwords` times\n","            labels.extend([label] * n_subwords)\n","\n","        input_ids = pad_sequences([CFG.tokenizer.convert_tokens_to_ids(tokenized_sentence)],\n","                                  maxlen=256, dtype=\"long\", value=0.0,\n","                                  truncating=\"post\", padding=\"post\")\n","\n","        tags = pad_sequences([[self.tag2idx.get(l) for l in labels]],\n","                             maxlen=256, value=self.tag2idx[\"PAD\"], padding=\"post\",\n","                             dtype=\"long\", truncating=\"post\")\n","\n","        attention_masks = [float(i != 0.0) for i in input_ids[0]]\n","\n","        return {\n","            \"ids\": torch.tensor(input_ids[0], dtype=torch.long),\n","            \"mask\": torch.tensor(attention_masks, dtype=torch.long),\n","            \"target_tag\": torch.tensor(tags[0], dtype=torch.long),\n","        }\n","\n","train_dataset = PunctuationDataset(texts=train_sentences, labels=train_labels,\n","                                   tag2idx=train_encoder)\n","valid_dataset = PunctuationDataset(texts=valid_sentences, labels=valid_labels,\n","                                   tag2idx=train_encoder)\n","\n","train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=True)\n","valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=CFG.batch_size, num_workers=4, shuffle=False)\n","\n","model = AutoModelForTokenClassification.from_pretrained('xlm-roberta-large',\n","                                                        num_labels=len(train_encoder),\n","                                                        output_attentions=False,\n","                                                        output_hidden_states=False)\n","\n","weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0]).cuda()\n","criterion = nn.CrossEntropyLoss(weight=weights)\n","\n","if CFG.full_finetuning:\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.0}\n","    ]\n","\n","else:\n","    param_optimizer = list(model.classifier.named_parameters())\n","    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","\n","optimizer = AdamW(\n","    optimizer_grouped_parameters,\n","    lr=CFG.learning_rate,\n","    eps=1e-8\n",")\n","\n","total_steps = len(train_data_loader) * CFG.epochs\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","\n","starting_epoch = 0\n","\n","if CFG.load_checkpoint:\n","    checkpoint = torch.load(CFG.checkpoint_path)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","    for state in optimizer.state.values():\n","        for k, v in state.items():\n","            if torch.is_tensor(v):\n","                state[k] = v.cuda()\n","\n","    starting_epoch = checkpoint['epoch'] + 1\n","\n","if torch.cuda.device_count() > 1:\n","    print(\"Using \", torch.cuda.device_count(), \"GPUs\")\n","    model = nn.DataParallel(model)\n","\n","loss_values, validation_loss_values = [], []\n","model.cuda()\n","\n","for epoch in range(starting_epoch, 8):\n","\n","    model.train()\n","    total_loss = 0\n","\n","    # Training loop\n","    tk0 = tqdm(train_data_loader, total=int(len(train_data_loader)), unit='batch')\n","    tk0.set_description(f'Epoch {epoch + 1}')\n","\n","    for step, batch in enumerate(tk0):\n","        # add batch to gpu\n","        for k, v in batch.items():\n","            batch[k] = v.to(CFG.device)\n","\n","        b_input_ids, b_input_mask, b_labels = batch['ids'], batch['mask'], batch['target_tag']\n","\n","        model.zero_grad()\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.apex, dtype=torch.bfloat16):\n","            outputs = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask, labels=b_labels)\n","            loss = criterion(outputs[1].view(-1, 6), b_labels.view(-1))\n","        loss.backward()\n","        total_loss += loss.item()\n","\n","        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=CFG.max_grad_norm)\n","\n","        optimizer.step()\n","\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_data_loader)\n","    print(\"Average train loss: {}\".format(avg_train_loss))\n","\n","    state = {\n","        'epoch': epoch,\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    }\n","    torch.save(state, checkpoint_folder + '/checkpoint_last.pt')\n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    model.eval()\n","    # Reset the validation loss for this epoch.\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    predictions, true_labels = [], []\n","\n","    best_val_loss = np.inf\n","\n","    for batch in tqdm(valid_data_loader, total=int(len(valid_data_loader)), unit='batch', leave=True):\n","        for k, v in batch.items():\n","            batch[k] = v.to(CFG.device)\n","        b_input_ids, b_input_mask, b_labels = batch['ids'], batch['mask'], batch['target_tag']\n","\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask, labels=b_labels)\n","        logits = outputs[1].detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences.\n","        eval_loss += criterion(outputs[1].view(-1, 6), b_labels.view(-1)).item()\n","        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","        true_labels.extend(label_ids)\n","\n","    eval_loss = eval_loss / len(valid_data_loader)\n","\n","    if eval_loss < best_val_loss:\n","        state = {\n","            'epoch': epoch,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","        }\n","        torch.save(state, checkpoint_folder + '/checkpoint_best.pt')\n","        best_val_loss = eval_loss\n","\n","    validation_loss_values.append(eval_loss)\n","    print(\"Validation loss: {}\".format(eval_loss))\n","\n","    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels) for p_i, l_i in zip(p, l) if\n","                 tag_values[l_i] != \"PAD\"]\n","    valid_tags = [tag_values[l_i] for l in true_labels for l_i in l if tag_values[l_i] != \"PAD\"]\n","\n","    val_accuracy = accuracy_score(valid_tags, pred_tags)\n","    val_f1_score = f1_score(valid_tags, pred_tags, average='macro')\n","    report = classification_report(valid_tags, pred_tags, output_dict=True, labels=np.unique(pred_tags))\n","\n","    df_report = pd.DataFrame(report).transpose()\n","    df_report['categories'] = list(df_report.index)\n","    df_report = df_report[ ['categories'] + [ col for col in df_report.columns if col != 'categories' ] ]\n","\n","    print(\"Validation Accuracy: {}\".format(val_accuracy))\n","    print(\"Validation F1-Score: {}\".format(val_f1_score))\n","    print(\"Classification Report: {}\".format(report))\n","\n","    df_report.to_csv(f'report_epoch{epoch + 1}.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}