model:
  model_name: /mnt/petrelfs/hanxiao/work/bengali_utils/wav2vec2-large-mms-1b-bengali
  # model_name: /mnt/petrelfs/hanxiao/work/lavis-kaggle/vigc/output/wav2vec_bengali_asr/baseline-large-0903/best_hf
  load_finetuned: False
  finetuned: ""
  post_process_flag: True
  arch: bengali_umongwav2vec
  model_type: default
  freeze_encoder: True
  loss_reduction: "sum"
  processor_name: "/mnt/petrelfs/hanxiao/work/bengali_utils/wav2vec2-xls-r-300m-bengali"
#  processor_name: arijitx/wav2vec2-xls-r-300m-bengali

datasets:
  # train
  wav2vec_filtered_seg_aug_asr:
    model_name: /mnt/petrelfs/hanxiao/work/bengali_utils/wav2vec2-large-mms-1b-bengali
    sample_ratio: 1
    seg_nums: 3
    ratio: 0.7

  wav2vec_filtered_concat_aug_asr:
    model_name: /mnt/petrelfs/hanxiao/work/bengali_utils/wav2vec2-large-mms-1b-bengali
    sample_ratio: 1
    seg_nums: 2
    ratio: 0.7

  # test
  wav2vec_bengali_asr_test:
    model_name: /mnt/petrelfs/hanxiao/work/bengali_utils/wav2vec2-large-mms-1b-bengali

run:
  runner: runner_iter
  task: whisper_bengali_asr_task
  # optimizer
  lr_sched: "linear_warmup_cosine_long_tail_lr"
  # init_lr: 3e-5
  # min_lr: 1e-5
  # warmup_lr: 1e-6
  init_lr: 1e-5
  min_lr: 5e-6
  warmup_lr: 1e-6

  weight_decay: 0.05

  batch_size_train: 4
  batch_size_eval: 4
  accum_grad_iters: 2

  num_workers: 4
  warmup_steps: 1000

  iters_per_inner_epoch: 1000
  max_iters: 60000

  seed: 42
  output_dir: "./output/wav2vec_bengali_asr"

  amp: True
  resume_ckpt_path: null

  evaluate: False
  train_splits: [ "train" ]
  valid_splits: [ "eval" ]

  device: "cuda"
  world_size: 1
  dist_url: "env://"
  distributed: True