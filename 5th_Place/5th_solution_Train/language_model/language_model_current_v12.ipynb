{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506ccee4-3f4b-4fe2-80e3-6bff0deb0ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "#import nltk\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from bnunicodenormalizer import Normalizer \n",
    "bnorm = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb0e50b-17ea-43dd-a591-a3ef25838d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnlp import NLTKTokenizer\n",
    "\n",
    "bnltk = NLTKTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fd08a44-2448-460e-b683-9925503ae21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def normalize_word(token):\n",
    "    _token = remove_punctuation(token) \n",
    "    _token = replace_numeric(_token, by_single_digit=True)\n",
    "    _token = '<num>' if _token == '#' else _token \n",
    "    return _token.strip().lower()\n",
    "\n",
    "\n",
    "def remove_punctuation(text, punctiation_extended=string.punctuation + \"\"\"\"„“‚‘\"\"\"):\n",
    "    return ''.join(c for c in text if c not in punctiation_extended)\n",
    "\n",
    "\n",
    "def replace_numeric(text, numeric_pattern=re.compile('[0-9]+'), digit_pattern=re.compile('[0-9]'), repl='#',\n",
    "                    by_single_digit=False):\n",
    "    return re.sub(numeric_pattern, repl, text) if by_single_digit else re.sub(digit_pattern, repl, text)\n",
    "\n",
    "\n",
    "def contains_numeric(text):\n",
    "    return any(char.isdigit() for char in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed178f27-344b-46d3-8e48-02fb0527e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ea6e9c-748b-4719-aee2-1970df0612f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.normalize import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eaac7c-dee6-4d23-9a8f-1ff215c6976f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tokenized and processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20ba9ec-8c7d-4b99-b6ec-d8aaf9c49e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ai4bharat_tokenized_processed():\n",
    "    '''\n",
    "    https://objectstore.e2enetworks.net/ai4b-public-nlu-nlg/indic-corp-frozen-for-the-paper-oct-2022/bn.txt\n",
    "    '''\n",
    "    i = 0\n",
    "    sentencelist = []\n",
    "    with open('base_files/bn.txt', 'r') as file:\n",
    "        for line in file:\n",
    "           #print(line, end='')  # Each line already ends with a newline character, so specify end='' to avoid printing double newlines\n",
    "            batch_sentence = bnltk.sentence_tokenize(line.strip()) \n",
    "            batch_sentence = [bnltk.word_tokenize(x) for x in batch_sentence]\n",
    "            batch_sentence = [[normalize_word(x) for x in f] for f in batch_sentence]\n",
    "            batch_sentence = [' '.join(w for w in sent if w).strip() for sent in batch_sentence]\n",
    "            sentencelist.append(batch_sentence)\n",
    "            i+=1\n",
    "            if i % 5000000 == 0:\n",
    "                print(f\"{i} samples are processed\")\n",
    "                #break\n",
    "    sentencelist = [item for sublist in sentencelist for item in sublist]\n",
    "    sentencelist = [line for line in sentencelist if line != '']\n",
    "    sentencelist = list(set(sentencelist))\n",
    "    print(f\"{len(sentencelist)} unique sentences\")\n",
    "    return sentencelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af15bd0-b5cc-4292-a76b-5ebaf5716845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000 samples are processed\n",
      "10000000 samples are processed\n",
      "15000000 samples are processed\n",
      "20000000 samples are processed\n",
      "25000000 samples are processed\n",
      "30000000 samples are processed\n",
      "35000000 samples are processed\n",
      "40000000 samples are processed\n",
      "62858539 unique sentences\n",
      "CPU times: user 2h 17min 43s, sys: 17.2 s, total: 2h 18min\n",
      "Wall time: 2h 18min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ekstep_sentences_tokenized_normalized = _ai4bharat_tokenized_processed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70350c67-6718-4a4e-8d13-c9cccff67b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ai4bharat_lm_tokenized_processed():\n",
    "    '''\n",
    "    https://storage.googleapis.com/vakyansh-open-models/language_model_text/bengali.zip\n",
    "    '''\n",
    "    i=0\n",
    "    sentencelist = []\n",
    "    with open(\"base_files/lm_ai4bharat.txt\", 'r', encoding='utf-8') as file:\n",
    "        for i,line in enumerate(file):\n",
    "            #sentencelist.append(line.strip())\n",
    "            batch_sentence = bnltk.sentence_tokenize(line.strip()) \n",
    "            batch_sentence = [bnltk.word_tokenize(x) for x in batch_sentence]\n",
    "            batch_sentence = [[normalize_word(x) for x in f] for f in batch_sentence]\n",
    "            batch_sentence = [' '.join(w for w in sent if w).strip() for sent in batch_sentence]\n",
    "            sentencelist.append(batch_sentence)\n",
    "            i+=1\n",
    "            if i % 5000000 == 0:\n",
    "                print(f\"{i} samples are processed\")\n",
    "    sentencelist = [item for sublist in sentencelist for item in sublist]\n",
    "    sentencelist = list(set(sentencelist))\n",
    "    print(f\"{len(sentencelist)} unique sentences\")\n",
    "    return sentencelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00b9c0c-1b0a-4c30-a194-43d27df4d6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000 samples are processed\n",
      "10000000 samples are processed\n",
      "15000000 samples are processed\n",
      "20000000 samples are processed\n",
      "25000000 samples are processed\n",
      "30000000 samples are processed\n",
      "30166470 unique sentences\n",
      "CPU times: user 52min 28s, sys: 7.02 s, total: 52min 35s\n",
      "Wall time: 52min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ai4bharat_data_tokenized_normalized = _ai4bharat_lm_tokenized_processed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b36b5d-e52b-4efa-b220-94cae44cab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lm_train_tokenized_processed():\n",
    "    '''\n",
    "    https://storage.googleapis.com/vakyansh-open-models/language_model_text/bengali.zip\n",
    "    '''\n",
    "    sentencelist = []\n",
    "    with open('base_files/lm_train.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            batch_sentence = bnltk.sentence_tokenize(line.strip()) \n",
    "            batch_sentence = [bnltk.word_tokenize(x) for x in batch_sentence]\n",
    "            batch_sentence = [[normalize_word(x) for x in f] for f in batch_sentence]\n",
    "            batch_sentence = [' '.join(w for w in sent if w).strip() for sent in batch_sentence]\n",
    "            sentencelist.append(batch_sentence)\n",
    "    sentencelist = [item for sublist in sentencelist for item in sublist]\n",
    "    sentencelist = list(set(sentencelist))\n",
    "    print(f\"{len(sentencelist)} unique sentences\")\n",
    "    return sentencelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bbaff84-cfae-41d0-a68a-ec249001157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152917 unique sentences\n",
      "CPU times: user 17.9 s, sys: 7.98 ms, total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lm_train_tokenized_processed = _lm_train_tokenized_processed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7744b718-a270-4777-9a01-20eb59d5d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _openslr_tokenized_processed():\n",
    "    '''\n",
    "    https://openslr.elda.org/resources/53/utt_spk_text.tsv\n",
    "    '''\n",
    "    sentencelist = []\n",
    "    with open('base_files/utt_spk_text.tsv', 'r') as file:\n",
    "        for line in file:\n",
    "            # Split the line by tabs\n",
    "            values = line.strip().split('\\t')\n",
    "            batch_sentence = bnltk.sentence_tokenize(values[-1].strip()) \n",
    "            batch_sentence = [bnltk.word_tokenize(x) for x in batch_sentence]\n",
    "            batch_sentence = [[normalize_word(x) for x in f] for f in batch_sentence]\n",
    "            batch_sentence = [' '.join(w for w in sent if w).strip() for sent in batch_sentence]\n",
    "            sentencelist.append(batch_sentence)\n",
    "    sentencelist = [item for sublist in sentencelist for item in sublist]\n",
    "    sentencelist = list(set(sentencelist))\n",
    "    print(f\"{len(sentencelist)} unique sentences\")\n",
    "    return sentencelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e354b476-7326-4c3c-9075-dc735206f598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109373 unique sentences\n",
      "CPU times: user 12.7 s, sys: 8 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "openslr_tokenized_processed = _openslr_tokenized_processed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "732d621b-6aa0-4603-ba03-e7873079b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kaggle_trains_tokenized_processed():\n",
    "    train_df = pd.read_csv(\"../data/train.csv\")\n",
    "    train_df_list = train_df['sentence'].tolist()\n",
    "    batch_sentence = [bnltk.word_tokenize(x) for x in train_df_list]\n",
    "    batch_sentence = [[normalize_word(x) for x in f] for f in batch_sentence]\n",
    "    batch_sentence = [' '.join(w for w in sent if w).strip() for sent in batch_sentence]\n",
    "    #train_df_final = [item for sublist in batch_sentence for item in sublist]\n",
    "    train_df_final = list(set(batch_sentence))\n",
    "    \n",
    "    df_dl = pd.read_csv(\"base_files/train_dl_sprint.csv\")\n",
    "    df_dl_list = df_dl['sentence'].tolist()\n",
    "    batch_sentence = [bnltk.word_tokenize(x) for x in df_dl_list]\n",
    "    batch_sentence = [[normalize_word(x) for x in f] for f in batch_sentence]\n",
    "    batch_sentence = [' '.join(w for w in sent if w).strip() for sent in batch_sentence]\n",
    "    #df_dl_final = [item for sublist in batch_sentence for item in sublist]\n",
    "    df_dl_final = list(set(batch_sentence))\n",
    "    \n",
    "    sentencelist = list(set(train_df_final+df_dl_final))\n",
    "    \n",
    "    print(f\"{len(sentencelist)} unique sentences\")\n",
    "    return sentencelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c4a2610-a286-4eeb-89c8-7ab1c58b6b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463124 unique sentences\n",
      "CPU times: user 1min 46s, sys: 80.1 ms, total: 1min 46s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kaggle_trains_tokenized_processed = _kaggle_trains_tokenized_processed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d76e63-6f25-405b-ac97-6f537c3308b9",
   "metadata": {},
   "source": [
    "# Saving single lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "358ff6ed-b260-442c-a9d9-d0737f16e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from bnunicodenormalizer import Normalizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00b2dc16-cc4d-4303-813b-0868565cac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_overview = {'kaggle_trains_tokenized_processed':kaggle_trains_tokenized_processed,\n",
    " 'ekstep_sentences_tokenized_normalized':ekstep_sentences_tokenized_normalized,\n",
    " 'ai4bharat_data_tokenized_normalized':ai4bharat_data_tokenized_normalized,\n",
    " 'lm_train_tokenized_processed':lm_train_tokenized_processed,\n",
    " 'openslr_tokenized_processed':openslr_tokenized_processed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b567327a-a955-4f2d-b6d8-a1f8251f10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_list(txt_overview):\n",
    "\n",
    "    chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\']'\n",
    "\n",
    "    for k,v in txt_overview.items():\n",
    "        with open(f'processed_files/{k}.txt', 'w') as f:\n",
    "            for sentence in tqdm(v):\n",
    "                sentence = normalizeUnicode(sentence.strip(), normalize_nukta=False)\n",
    "                f.write(f\"{sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c261b91-5bd9-4759-b07e-bb4a7a6277ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 463124/463124 [00:02<00:00, 179161.79it/s]\n",
      "100%|███████████████████████████| 62858539/62858539 [08:30<00:00, 123177.01it/s]\n",
      "100%|███████████████████████████| 30166470/30166470 [04:17<00:00, 117098.09it/s]\n",
      "100%|███████████████████████████████| 152917/152917 [00:00<00:00, 286906.39it/s]\n",
      "100%|███████████████████████████████| 109373/109373 [00:00<00:00, 368129.61it/s]\n",
      "100%|███████████████████████████| 27810072/27810072 [03:05<00:00, 149823.01it/s]\n",
      "100%|█████████████████████████████| 6715229/6715229 [01:05<00:00, 101749.35it/s]\n",
      "100%|█████████████████████████████| 2182535/2182535 [00:15<00:00, 137385.54it/s]\n",
      "100%|█████████████████████████████| 8064627/8064627 [00:50<00:00, 161126.30it/s]\n",
      "100%|████████████████████████████| 11007401/11007401 [04:36<00:00, 39808.72it/s]\n",
      "100%|███████████████████████████| 23784452/23784452 [03:15<00:00, 121698.40it/s]\n"
     ]
    }
   ],
   "source": [
    "_save_list(txt_overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed9e37-4a20-4c41-8fea-dc17fd4330fd",
   "metadata": {},
   "source": [
    "# Creating LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fb4be2-1835-4a11-b22d-33a2290c0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19184044-e3f8-408d-a640-eb50286fd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_overview = ['kaggle_trains_tokenized_processed',\n",
    " 'ekstep_sentences_tokenized_normalized',\n",
    " 'ai4bharat_data_tokenized_normalized',\n",
    " 'lm_train_tokenized_processed',\n",
    " 'openslr_tokenized_processed'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb5d0b9-ebc8-43f4-9842-fd729904c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_processed(sourcename):\n",
    "    i = 0\n",
    "    sentencelist = []\n",
    "    with open(f'processed_files/{sourcename}.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            sentencelist.append(line.strip())\n",
    "            #if i % 1000 == 0:\n",
    "            #    break\n",
    "    return sentencelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4aea3e3-97f8-42ed-bcc8-bd7846d8722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle_trains_tokenized_processed\n",
      "ekstep_sentences_tokenized_normalized\n",
      "ai4bharat_data_tokenized_normalized\n",
      "lm_train_tokenized_processed\n",
      "openslr_tokenized_processed\n"
     ]
    }
   ],
   "source": [
    "all_sentence_list = {}\n",
    "\n",
    "for source in source_overview:\n",
    "    print(source)\n",
    "    all_sentence_list[source] = _read_processed(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bac166b2-d2c0-4725-b976-cdd7df56da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all_sentences = (all_sentence_list['kaggle_trains_tokenized_processed']+\n",
    "                    all_sentence_list['ekstep_sentences_tokenized_normalized']+\n",
    "                    all_sentence_list['ai4bharat_data_tokenized_normalized']+\n",
    "                    all_sentence_list['lm_train_tokenized_processed']+\n",
    "                    all_sentence_list['openslr_tokenized_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7ad0df7-2914-4b9f-855a-8dbb9674d58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93750423"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "808a953d-001c-47df-aa91-a8e7197ce063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89885456"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_all_sentences = list(set(new_all_sentences))\n",
    "len(new_all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf2a7d4e-b55c-49ee-b337-7e62bf6823ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lm_model_dw(all_sentences):\n",
    "    #import re\n",
    "\n",
    "    chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\']'\n",
    "\n",
    "    with open('text_full_v12_base.txt', 'w') as f:\n",
    "        for sentence in tqdm(all_sentences):\n",
    "            sentence = normalizeUnicode(sentence.strip(), normalize_nukta=False)\n",
    "            f.write(f\"{sentence}\\n\")\n",
    "\n",
    "            #f.write(sentence)\n",
    "            #f.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac562161-e145-4747-bf0a-ce1378f60b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 89885456/89885456 [12:25<00:00, 120542.06it/s]\n"
     ]
    }
   ],
   "source": [
    "_lm_model_dw(new_all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9217832c-7daa-4f30-b47d-bf72166df538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /media/benedikt/T7/text_full_v12_base.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1231276442 types 8367842\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:100414104 2:7891496448 3:14796556288 4:23674488832 5:34525298688\n",
      "Statistics:\n",
      "1 8367842 D1=0.727727 D2=1.04105 D3+=1.29796\n",
      "2 151674141 D1=0.757378 D2=1.0847 D3+=1.34373\n",
      "3 514674041 D1=0.839338 D2=1.1802 D3+=1.36572\n",
      "4 118883901/772172452 D1=0.904396 D2=1.31585 D3+=1.44292\n",
      "5 96109758/845649270 D1=0.846464 D2=1.46157 D3+=1.73356\n",
      "Memory estimate for binary LM:\n",
      "type       MB\n",
      "probing 19829 assuming -p 1.5\n",
      "probing 24354 assuming -r models -p 1.5\n",
      "trie    11423 without quantization\n",
      "trie     6760 assuming -q 8 -b 8 quantization \n",
      "trie     9446 assuming -a 22 array pointer compression\n",
      "trie     4783 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:100414104 2:2426786256 3:10293480820 4:2853213624 5:2691073224\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "#**********#########################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:100414104 2:2426786256 3:10293480820 4:2853213624 5:2691073224\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:79360988 kB\tVmRSS:3540 kB\tRSSMax:55809044 kB\tuser:1332.3\tsys:463.854\tCPU:1796.15\treal:2256\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 5 --prune 0 0 0 1 1 -S 60% < \"text_full_v12_base.txt\" > \"5gram.arpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a9e7333-8faa-48f2-af7d-e2e2a73d03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n",
    "    has_added_eos = False\n",
    "    for line in read_file:\n",
    "        if not has_added_eos and \"ngram 1=\" in line:\n",
    "            count=line.strip().split(\"=\")[-1]\n",
    "            write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "        elif not has_added_eos and \"<s>\" in line:\n",
    "            write_file.write(line)\n",
    "            write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "            has_added_eos = True\n",
    "        else:\n",
    "            write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99a565e5-3ba7-4023-a612-a5fc4c534a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /media/benedikt/T7/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/build_binary -S 60% 5gram_correct.arpa 5gram.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e273435-0c10-4f27-9ac8-c480af8574d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /media/benedikt/T7/5gram_correct.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Unigrams and labels don't seem to agree.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import BeamSearchDecoderCTC\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,Wav2Vec2ProcessorWithLM\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from bnunicodenormalizer import Normalizer \n",
    "import librosa\n",
    "from jiwer import wer\n",
    "from transformers import Wav2Vec2ProcessorWithLM,pipeline\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"../experiments/runs/wav2vec_indic_v35/processor\")\n",
    "\n",
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "\n",
    "vocab_dict = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(vocab_dict.keys()),\n",
    "    kenlm_model_path='5gram_correct.arpa',\n",
    "    #alpha=0.2, #alpha 0.2 and beta 0.5 ->\n",
    "    #beta=0.2\n",
    ")\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder,\n",
    ")\n",
    "\n",
    "processor.save_pretrained(\"lms/new_model_arpa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58a904d1-11ee-4e6f-a3e1-06238061f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "No known unigrams provided, decoding results might be a lot worse.\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import BeamSearchDecoderCTC\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor,Wav2Vec2ProcessorWithLM\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from bnunicodenormalizer import Normalizer \n",
    "import librosa\n",
    "from jiwer import wer\n",
    "from transformers import Wav2Vec2ProcessorWithLM,pipeline\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"../experiments/runs/wav2vec_indic_v35/processor\")\n",
    "\n",
    "\n",
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "\n",
    "vocab_dict = {k: v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(vocab_dict.keys()),\n",
    "    kenlm_model_path='5gram.binary',\n",
    "    #alpha=0.2, #alpha 0.2 and beta 0.5 ->\n",
    "    #beta=0.2\n",
    ")\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder,\n",
    ")\n",
    "\n",
    "processor.save_pretrained(\"lms/new_model_bin_mixed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
