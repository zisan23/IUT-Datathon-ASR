{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Whipser Inference 🎵👂➡️🇧🇩📝\n\nThis uses 2x T4 GPUs for distributed inference. The trained model came from this notebook: https://www.kaggle.com/code/nbroad/whisper-training-starter-kit\n\n\nInference Notebook Version | Training Notebook Version | Model | CV WER | LB WER\n- | - | - | - | -\n | 1 | openai/whisper-base | 0.69 | \n9 | 2 | bangla-speech-processing/BanglaASR | 0.529 | 0.664\n11 | - | bangla-speech-processing/BanglaASR | 0.244 | 0.644\n12 | - | bangla-speech-processing/BanglaASR | 0.386 | 0.638\n14 | - | bangla-speech-processing/BanglaASR | 0.419 | 0.629\n18 | - | IndicWhisper-bn | ? | 0.542\n\nNotebook Version 11 used a model trained on 84k samples from the given dataset. This was overfitting.  \nNotebook Version 12 used a model trained on a different set of 90k samples. It also includes postprocessing code. Still overfitting so better CV is needed.  \nNotebook Version 14 trained on all samples.  \nNotebook Version 18 used the [Bengali version of IndicWhisper](https://github.com/AI4Bharat/vistaar) without any training on the given dataset. Tried adding `do_normalize=True` and CV was worse.\n\nIt takes about 2 hours to submit and score for whisper small, 2.5 hours for whisper medium\n\nA couple versions failed because it was doing preprocessing in chunks. Now it does it in each batch.\nVersions 15-17 failed because it was predicting empty string. See here for details on this scoring error.\n\nI will update this file with speed optimizations in the near future.\n- [Update] BetterTransformer does not seem to help.","metadata":{}},{"cell_type":"code","source":"!pip install datasets --no-index --find-links=file:///kaggle/input/hf-ds -U -q\n    \n!cp -r /kaggle/input/python-packages2 /tmp\n!tar xvfz /tmp/python-packages2/normalizer.tgz\n!pip install ./normalizer/bnunicodenormalizer-0.0.24.tar.gz -f ./ --no-index\n\n\n!tar xvfz /tmp/python-packages2/jiwer.tgz\n!pip install ./jiwer/python-Levenshtein-0.12.2.tar.gz -f ./ --no-index\n!pip install ./jiwer/jiwer-2.3.0-py3-none-any.whl -f ./ --no-index","metadata":{"execution":{"iopub.status.busy":"2023-08-03T00:49:33.489742Z","iopub.execute_input":"2023-08-03T00:49:33.490173Z","iopub.status.idle":"2023-08-03T00:49:38.794555Z","shell.execute_reply.started":"2023-08-03T00:49:33.490125Z","shell.execute_reply":"2023-08-03T00:49:38.791391Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"jiwer/\njiwer/jiwer-2.3.0-py3-none-any.whl\njiwer/python-Levenshtein-0.12.2.tar.gz\njiwer/setuptools-65.3.0-py3-none-any.whl\nLooking in links: ./\nProcessing ./jiwer/jiwer-2.3.0-py3-none-any.whl\nINFO: pip is looking at multiple versions of jiwer to determine which version is compatible with other requirements. This could take a while.\n\u001b[31mERROR: Could not find a version that satisfies the requirement python-Levenshtein==0.12.2 (from jiwer) (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for python-Levenshtein==0.12.2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    \n    model_path = \"/kaggle/input/indicwhisper-bn/bengali_models/whisper-medium-bn_alldata_multigpu\"\n    batch_size = 8\n    do_eval = False\n    num_eval = 500\n    do_predict = True\n    do_normalize = False","metadata":{"execution":{"iopub.status.busy":"2023-08-03T00:56:09.152636Z","iopub.execute_input":"2023-08-03T00:56:09.153021Z","iopub.status.idle":"2023-08-03T00:56:09.158988Z","shell.execute_reply.started":"2023-08-03T00:56:09.152982Z","shell.execute_reply":"2023-08-03T00:56:09.157898Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"%%writefile infer.py\n\nimport os\nimport sys\nimport random\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Union\n\nimport datasets\nimport torch\nimport numpy as np\nfrom datasets import Dataset\n\nfrom transformers import (\n    AutoConfig,\n    AutoFeatureExtractor,\n    AutoModelForSpeechSeq2Seq,\n    AutoProcessor,\n    AutoTokenizer,\n    HfArgumentParser,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    set_seed,\n)\n\n@dataclass\nclass Config:\n    model_name_or_path: str = field(\n        metadata={\n            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n        }\n    )\n    audio_column_name: str = field(\n        default=\"audio\",\n        metadata={\n            \"help\": \"The name of the dataset column containing the audio data. Defaults to 'audio'\"\n        },\n    )\n    num_workers: int = field(\n        default=2,\n        metadata={\n            \"help\": \"The number of workers for preprocessing\"\n        },\n    )\n    use_bettertransformer: bool = field(default=False, metadata={\n            \"help\": \"Use BetterTransformer (https://huggingface.co/docs/optimum/bettertransformer/overview)\"\n        }\n    num_eval: int = field(default=1000, metadata={\n            \"help\": \"The number of samples to run for CV\"\n        })\n    do_normalize: bool = field(default=False, metadata={\n            \"help\": \"Normalize in the feature extractor\"\n        })\n\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor ([`WhisperProcessor`])\n            The processor used for processing the data.\n        decoder_start_token_id (`int`)\n            The begin-of-sentence of the decoder.\n        forward_attention_mask (`bool`)\n            Whether to return attention_mask.\n    \"\"\"\n\n    processor: Any\n    decoder_start_token_id: int\n    forward_attention_mask: bool\n    audio_column_name: str\n    do_normalize: bool\n\n    def __call__(\n        self, features\n    ) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need\n        # different padding methods\n        \n        model_input_name = self.processor.model_input_names[0]\n        \n        features = [\n            prepare_dataset(\n                feature, \n                audio_column_name=self.audio_column_name, \n                model_input_name=model_input_name,\n                feature_extractor=self.processor.feature_extractor,\n                do_normalize=self.do_normalize\n            ) for feature in features\n        ]\n        \n        input_features = [\n            {model_input_name: feature[model_input_name]} for feature in features\n        ]\n\n        batch = self.processor.feature_extractor.pad(\n            input_features, return_tensors=\"pt\"\n        )\n\n        return batch\n\ndef prepare_dataset(batch, audio_column_name, model_input_name, feature_extractor, do_normalize):\n    # process audio\n    sample = batch[audio_column_name]\n\n    # if longer than 30 seconds, truncate.\n    # for best score, break long files up\n    if len(sample[\"array\"]) > (16000 * 30):\n        sample[\"array\"] = sample[\"array\"][:16000 * 30]\n\n    inputs = feature_extractor(\n        sample[\"array\"],\n        sampling_rate=sample[\"sampling_rate\"],\n        do_normalize=do_normalize,\n    )\n    # process audio length\n    batch[model_input_name] = inputs.get(model_input_name)[0]\n\n    return batch\n\n\n\ndef main():\n\n    parser = HfArgumentParser((Config, Seq2SeqTrainingArguments))\n\n    cfg, training_args = parser.parse_args_into_dataclasses()\n\n    # Set seed before initializing model.\n    set_seed(training_args.seed)\n\n    config = AutoConfig.from_pretrained(\n        cfg.model_name_or_path,\n    )\n\n    feature_extractor = AutoFeatureExtractor.from_pretrained(\n        cfg.model_name_or_path,\n    )\n    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n        cfg.model_name_or_path,\n        config=config,\n    )\n    \n    # BetterTransformer does not provide any noticeable speed up in fp32\n    if cfg.use_bettertransformer:\n        sys.path.append(\"/kaggle/input/hugging-face-optimum\")\n        from optimum.bettertransformer import BetterTransformer\n        \n        model = BetterTransformer.transform(model)\n        \n        if training_args.local_process_index == 0:\n            print(\"Converted to BetterTransformer\")\n\n    processor = AutoProcessor.from_pretrained(cfg.model_name_or_path)\n    \n    if training_args.do_eval:\n        data_dir = \"/kaggle/input/bengaliai-speech/train_mp3s\"\n    else:\n        data_dir = \"/kaggle/input/bengaliai-speech/test_mp3s\"\n    \n    audio_files = list(map(str, Path(data_dir).glob(\"*.mp3\")))\n    \n    if training_args.do_eval:\n        audio_files = random.sample(audio_files, cfg.num_eval)\n    \n    ds = Dataset.from_dict({\"audio\": audio_files})\n    \n    ds = ds.map(lambda x: {\"id\": Path(x[\"audio\"]).stem, \"filesize\": os.path.getsize(x[\"audio\"])}, num_proc=cfg.num_workers)\n    \n    ds = ds.cast_column(\n        cfg.audio_column_name,\n        datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate),\n    )\n    \n    # sort by filesize to minimize padding\n    ds = ds.sort(\"filesize\")\n    ds = ds.add_column(\"idx\", range(len(ds)))\n    \n    # save ids\n    ds.remove_columns([x for x in ds.column_names if x != \"id\"]).to_json(\"ids.json\")\n    \n    model_input_name = feature_extractor.model_input_names[0]\n    \n    \n    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n        processor=processor,\n        decoder_start_token_id=model.config.decoder_start_token_id,\n        forward_attention_mask=False,\n        audio_column_name=cfg.audio_column_name,\n        do_normalize=cfg.do_normalize,\n    )\n\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        tokenizer=feature_extractor,\n        data_collator=data_collator,\n    )\n    \n    # Probably not necessary to do in chunks, but keeping for the time being\n    chunk_size = 250\n    \n    for num, i in enumerate(range(0, len(ds), chunk_size)):\n        ii = min(i+chunk_size, len(ds))\n        temp = ds.select(range(i, ii))\n        \n        predictions = trainer.predict(temp).predictions\n    \n        Dataset.from_dict({\"idx\": temp[\"idx\"]}).to_json(f\"vectorized_idxs_{num}.json\")\n        np.save(f\"preds_{num}.npy\", predictions)\n    \n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-03T00:56:11.374435Z","iopub.execute_input":"2023-08-03T00:56:11.374809Z","iopub.status.idle":"2023-08-03T00:56:11.387580Z","shell.execute_reply.started":"2023-08-03T00:56:11.374779Z","shell.execute_reply":"2023-08-03T00:56:11.386106Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Overwriting infer.py\n","output_type":"stream"}]},{"cell_type":"code","source":"if CFG.do_eval:\n    !torchrun --nproc_per_node 2 infer.py \\\n      --model_name_or_path $CFG.model_path \\\n      --report_to \"none\" \\\n      --dataloader_num_workers 1 \\\n      --per_device_eval_batch_size $CFG.batch_size \\\n      --predict_with_generate \\\n      --output_dir \"./\" \\\n      --remove_unused_columns False \\\n      --do_eval True \\\n      --num_eval $CFG.num_eval \\\n      --do_normalize $CFG.do_normalize\n\nif CFG.do_predict:\n    !torchrun --nproc_per_node 2 infer.py \\\n      --model_name_or_path $CFG.model_path \\\n      --report_to \"none\" \\\n      --dataloader_num_workers 1 \\\n      --per_device_eval_batch_size $CFG.batch_size \\\n      --predict_with_generate \\\n      --output_dir \"./\" \\\n      --remove_unused_columns False \\\n      --do_normalize $CFG.do_normalize","metadata":{"execution":{"iopub.status.busy":"2023-08-03T00:56:24.185188Z","iopub.execute_input":"2023-08-03T00:56:24.185639Z","iopub.status.idle":"2023-08-03T01:05:43.749058Z","shell.execute_reply.started":"2023-08-03T00:56:24.185608Z","shell.execute_reply":"2023-08-03T01:05:43.747809Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nCreating json from Arrow format: 100%|████████████| 1/1 [00:00<00:00, 34.01ba/s]\n\n100%|███████████████████████████████████████████| 16/16 [02:40<00:00, 10.05s/it]\nCreating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 257.86ba/s]\nCreating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 270.01ba/s]\n100%|███████████████████████████████████████████| 16/16 [03:58<00:00, 14.90s/it]\nCreating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 246.33ba/s]\nCreating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 228.29ba/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create submission.csv file","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom itertools import chain\n\nfrom transformers import AutoTokenizer\nfrom bnunicodenormalizer import Normalizer\n\n\nall_ids = [x.stem for x in Path(\"/kaggle/input/bengaliai-speech/test_mp3s\").glob(\"*.mp3\")]\n\ntokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n\n\ntext_preds = []\n\n\nfor f in sorted(Path(\"./\").glob(\"preds*.npy\")):\n    preds = np.load(f)\n    text_preds.extend(tokenizer.batch_decode(preds, skip_special_tokens=True))\n\n# ids.json has the original order\nids = []\nwith open(\"ids.json\") as fp:\n    for line in fp:\n        ids.append(json.loads(line)[\"id\"])\n\npred_idxs = []\n\nfor fname in sorted(Path(\"./\").glob(\"vectorized_*.json\")):\n    # vectorized_idxs.json has the sorted version\n    with open(fname) as fp:\n        for line in fp:\n            pred_idxs.append(json.loads(line)[\"idx\"])\n\npred_df = pd.DataFrame({\"idx\": pred_idxs, \"sentence\": text_preds})\nid_df = pd.DataFrame({\"id\": ids})\nid_df[\"idx\"] = range(len(id_df))\n\n# merge dataframes to match ids and sentences properly\npred_df = pred_df.merge(id_df, on=\"idx\").drop(columns=[\"idx\"])\n\npred_ids = pred_df.id\n\nif CFG.do_predict:\n    missing_ids = set(all_ids) - set(pred_ids)\n    if len(missing_ids) > 0:\n        temp = pd.DataFrame({\"id\": list(missing_ids)})\n        temp[\"sentence\"] = \"।\"\n\n        pred_df = pd.concat([\n            pred_df, \n            temp,\n        ],\n            axis=0\n        )\n    \npred_df[\"sentence\"].fillna(\"।\", inplace=True)\n\n# Post-processing\n# from: https://www.kaggle.com/code/reasat/yellowking-dlsprint-inference?scriptVersionId=137162907&cellId=19\nbnorm = Normalizer()\n\ndef postprocess(sentence):\n    _words = [bnorm(word)['normalized']  for word in sentence.split()]\n    sentence = \" \".join([word for word in _words if word is not None])\n    try:\n        if sentence[-1]!=\"।\":\n            sentence+=\"।\"\n    except:\n        print(sentence)\n    return sentence\n\npred_df[\"sentence\"] = [postprocess(s) for s in pred_df[\"sentence\"]]\npred_df[\"sentence\"] = [x if len(x) > 0 else \"।\" for x in pred_df[\"sentence\"]]\n\npred_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:05:45.755426Z","iopub.execute_input":"2023-08-03T01:05:45.755833Z","iopub.status.idle":"2023-08-03T01:05:48.192810Z","shell.execute_reply.started":"2023-08-03T01:05:45.755798Z","shell.execute_reply":"2023-08-03T01:05:48.191731Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"pred_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:05:49.516626Z","iopub.execute_input":"2023-08-03T01:05:49.516997Z","iopub.status.idle":"2023-08-03T01:05:49.528477Z","shell.execute_reply.started":"2023-08-03T01:05:49.516967Z","shell.execute_reply":"2023-08-03T01:05:49.527434Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                      sentence            id\n0               আছাড়াছাড়াছাড়া।  964c0abe4385\n1            ভিত্তি হন ও তিনি।  0b2954c40b8f\n2                         এটি।  63e0b415bfab\n3  তোমসে জেখিলে প্রাপ্ত মিটিং।  fb2774c3c552\n4              এই সম্পর্ক আছে।  8a5875d54a61","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>আছাড়াছাড়াছাড়া।</td>\n      <td>964c0abe4385</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ভিত্তি হন ও তিনি।</td>\n      <td>0b2954c40b8f</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>এটি।</td>\n      <td>63e0b415bfab</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>তোমসে জেখিলে প্রাপ্ত মিটিং।</td>\n      <td>fb2774c3c552</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>এই সম্পর্ক আছে।</td>\n      <td>8a5875d54a61</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"if CFG.do_eval:\n    \n    train_df = pd.read_csv(\"/kaggle/input/bengaliai-speech/train.csv\")\n    \n    train_df[\"true\"] = train_df[\"sentence\"]\n    \n    pred_df = pred_df.merge(train_df[[\"true\", \"id\"]], on=\"id\", how=\"left\")\n    \n    from jiwer import wer\n    \n    pred_df[\"wer\"] = [wer(pred, gt) for pred, gt in pred_df[[\"sentence\", \"true\"]].values]\n    \n    print(round(pred_df[\"wer\"].mean(), 4))","metadata":{"execution":{"iopub.status.busy":"2023-08-03T01:05:52.329839Z","iopub.execute_input":"2023-08-03T01:05:52.330224Z","iopub.status.idle":"2023-08-03T01:05:58.484151Z","shell.execute_reply.started":"2023-08-03T01:05:52.330193Z","shell.execute_reply":"2023-08-03T01:05:58.482974Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"0.736\n","output_type":"stream"}]}]}